<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>
  <head>
  <meta name="google-site-verification" content="xDNWUvx6Q5EWK5YYSyKvK8DZTmvXhKsGX203Ll-BFFE" >	
  <meta name=viewport content=“width=800”>
  <meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
  <meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
  <style type="text/css">
  @import url(https://fonts.googleapis.com/css?family=Roboto:400,400italic,500,500italic,700,700italic,900,900italic,300italic,300);
    /* Color scheme stolen from Sergey Karayev */
    a {
    /*color: #b60a1c;*/
    color: #1772d0;
    /*color: #bd0a36;*/
    text-decoration:none;
    }
    a:focus, a:hover {
    color: #f09228;
    text-decoration:none;
    }
    body,td,th,tr,p,a {
    font-family: 'Roboto', sans-serif;
    font-size: 15px;
    font-weight: 300;
    }
    strong {
    font-family: 'Roboto', sans-serif;
    /*font-family: 'Lato', Verdana, Helvetica, sans-serif;*/
    /*font-family: 'Avenir Next';*/
    font-size: 15px;
    font-weight: 400;
    }
    heading {
    /*font-family: 'Lato', Verdana, Helvetica, sans-serif;*/
    font-family: 'Roboto', sans-serif;
    /*font-family: 'Avenir Next';*/
    /*src: url("./fonts/Roboto_Mono_for_Powerline.ttf");*/
    font-size: 24px;
    font-weight: 400;
    }
    papertitle {
    /*font-family: 'Lato', Verdana, Helvetica, sans-serif;*/
    font-family: 'Roboto', sans-serif;
    /*font-family: 'Avenir Next';*/
    /*src: url("./fonts/Roboto_Mono_for_Powerline.ttf");*/
    font-size: 15px;
    font-weight:500;
    }
    name {
    /*font-family: 'Lato', Verdana, Helvetica, sans-serif;*/
    font-family: 'Roboto', sans-serif;
    /*font-family: 'Avenir Next';*/
    font-weight: 400;
    font-size: 32px;
    }
    .one
    {
    width: 160px;
    height: 140px;
    position: relative;
    }
    .two
    {
    width: 160px;
    height: 160px;
    position: absolute;
    transition: opacity .2s ease-in-out;
    -moz-transition: opacity .2s ease-in-out;
    -webkit-transition: opacity .2s ease-in-out;
    }
    .fade {
     transition: opacity .2s ease-in-out;
     -moz-transition: opacity .2s ease-in-out;
     -webkit-transition: opacity .2s ease-in-out;
    }
    span.highlight {
        background-color: #ffffd0;
    }
  </style>
  <link rel="icon" type="image/png" href="media/icon.png">
  <title>Tao Pu (蒲韬) - Homepage</title>
  <meta http-equiv="Content-Type" content="text/html; charset=us-ascii">
  <link href='https://fonts.googleapis.com/css?family=Lato:400,700,400italic,700italic' rel='stylesheet' type='text/css'>
  <script src="script/functions.js"></script>

  <script>
    // 实现 toggleblock 函数
    function toggleblock(id) {
        var element = document.getElementById(id);
        if (element.style.display === "none") {
            element.style.display = "block";
        } else {
            element.style.display = "none";
        }
    }
  </script>

  </head>

  <body>
  <table width="800" border="0" align="center" cellspacing="0" cellpadding="0">
    <tr>
    <td>
      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tr>
        <td width="67%" valign="middle">
        <p align="center">
          <name>Tao PU (蒲韬)</name>
        </p>
        <p>
          I am currently a Ph.D. student from the School of Computer Science and Engineering at Sun Yat-sen University. I am grateful to join the <a href="http://www.sysu-hcp.net/">HCP Lab</a> and be advised by Prof. <a href="http://www.linliang.net/">Liang Lin</a>. Before that, I received my Bachelor’s Degree from Sun Yat-sen University in 2020.
        </p> 
        <p>
          I’m broadly interested in visual understanding. The goal of my research is to build general-purpose agents that can abstract the environment from a human-oriented perspective and have versatile motor skills in challenging scenarios.
        </p>  
        <p align=center>
          <a href="mailto:putao537@gmail.com">Email</a> &nbsp|&nbsp
          <a href="files/resume.pdf">CV</a> &nbsp|&nbsp
          <a href="https://github.com/putao537">GitHub</a> &nbsp|&nbsp
          <a href="https://scholar.google.com/citations?user=cBRGYzYAAAAJ&hl=en">Google Scholar</a> &nbsp|&nbsp
          <a href="https://www.linkedin.com/in/tao-pu-7353a0180/"> LinkedIn</a>
        </p>
        </td>
        <td width="33%">
          <img src="media/profile.jpg" width="250" alt="headshot">
        </td>

      </tr>
      
      </table>

      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
      <tr>
        <td>
          <heading>News</heading>
            <ul>
              <li><strong>09/2023</strong> Join <a href="https://pku.ai/"> PKU CoRe Lab</a> as a visiting student! Worked with <a href="https://jiemingcui.github.io/">Jieming Cui</a>, and advised by Dr. <a href="https://yzhu.io/"> Yixin Zhu</a>.</li>
              <!-- 
              <li><strong>07/2024</strong> Our paper <a href="https://segment3d.github.io/"><strong>Segment3D</strong></a> is accepted to ECCV 2024! Also, I will be co-organizing two workshops: <a href="https://focus-workshop.github.io/">Foundation Models Creators Meet Users (FOCUS)</a> and <a href="https://opensun3d.github.io/">Open-Vocabulary 3D Scene Understanding (OpenSUN3D)</a>.</li>
              <li><strong>05/2024</strong> <b><span style="color:#c20000;">Career Update</span></b>: I start working as a research scientist at <a href="https://deepmind.google/"><strong>Google DeepMind</strong></a>!</li>
              <li><strong>03/2024</strong> Our papers <a href="https://rwn17.github.io/nerf-on-the-go/"><strong>NeRF <em>On-the-go</em></strong></a> and <a href="https://neural-edge-map.github.io/"><strong>3D Neural Edge Reconstruction</strong></a> are accepted to <strong>CVPR 2024</strong>! Congrats on the successful master theses of my amazing students at ETH Zurich, <a href="https://scholar.google.com/citations?user=A6d9VTAAAAAJ">Lei Li</a> and <a href="https://github.com/rwn17">Weining Ren</a>!</li>
              <li><strong>03/2024</strong> <a href="https://nicer-slam.github.io/"><strong>NICER-SLAM</strong></a> received the <img src="media/logo_award.png" width="20"><strong><span style="color:#c20000;">Best Paper Honorable Mention Award</span></strong> at <strong>3DV 2024</strong>! Congrats to all co-authors, especially <a href="https://zzh2000.github.io/">Zihan</a>, who started this project during his bachelor and did such a wonderful job!!</li>
              <li><strong>03/2024</strong> Invited to give a talk on <a href="files/talk_hku.pdf">2D Magic in a 3D World</a> at Imperial College London and the University of Hong Kong. </li>
              <li><strong>11/2023</strong> <strong>I successfully defended my PhD</strong>! [<a href="files/Songyou_PhD_Thesis.pdf">Thesis</a>][<a href="files/songyou_phd_defense.pdf">Defense Slides</a>]
              <li><strong>10/2023</strong> Our papers <img src="media/logo_nicer.png" width="20"><a href="https://nicer-slam.github.io"> <strong>NICER-SLAM</strong></a> and <a href="https://l1346792580123.github.io/nccsfs/"><strong>FastHuman</strong></a> are accepted to 3DV 2024.
              <li><strong>07/2023</strong> I served as an Area Chair at <a href="https://3dvconf.github.io/2024/area-chairs/">3DV 2024</a>.
              <li><strong>07/2023</strong> I received the <img src="media/logo_award.png" width="20"><a href="https://twitter.com/GMFarinella/status/1679889056266694666"><strong>Best Presentation Award</strong></a> at <a href="https://iplab.dmi.unict.it/icvss2023/">ICVSS 2023</a>!</li>
              <li><strong>07/2023</strong> Our paper <a href="https://primecai.github.io/diffdreamer">DiffDreamer</a> is accepted to ICCV 2023! Congrats Shengqu on a successful master thesis!
              <li><strong>07/2023</strong> Invited to give a 90-min lecture at <a href="https://sgp2023.github.io/program/">SGP 2023</a> graduate school in <a href="files/talk_sgp.pdf">neural explicit-implicit representations!</a></li>
              -->
            </ul>
            <a href="javascript:toggleblock('new')">---- show more ----</a>
              <div id="new" style="display: none;">
                <ul>
                </ul>
              </div>
        </td>
      </tr>

      <!-- Research -->
      <tr>
        <td width="100%" valign="middle">
          <heading>Selected Publications</heading> <br>
          Please see <a href="./publication.html" target="_blank">this page</a> for more recent works and arXiv papers. <br>
          * denotes <strong>equal contribution</strong>,  &#8224; denotes <strong>corresponding author</strong>.
        </td>
      </tr>
      </table>

      <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20" >
      <!--
        <tr onmouseout="TIP2024_STKET_stop()" onmouseover="TIP2024_STKET_start()">  
          <td width="25%">
            <div class="one">
            <div class="two" id = 'TIP2024_STKET'>
              <img src='media/TIP2024_STKET.png' width="160" height="124"></div>
            <img src='media/TIP2024_STKET.png' width="160" height="124"></div>
            <script type="text/javascript">
            function TIP2024_STKET_start() { 
            document.getElementById('TIP2024_STKET').style.opacity = "1";
            }
            function TIP2024_STKET_stop() { 
            document.getElementById('TIP2024_STKET').style.opacity = "0"; 
            }
            TIP2024_STKET_stop()
            </script>
            </script>
          </td>
          <td valign="top" width="75%">
                <papertitle>
                  Spatial–Temporal Knowledge-Embedded Transformer for Video Scene Graph Generation
                </papertitle>
              </a>
          <br>
              <strong>Tao Pu</strong>, Tianshui Chen &#9993;, Hefeng Wu, Yongyi Lu, Liang Lin
          <br>
              <em>IEEE Transactions on Image Processing (<strong>TIP</strong>)</em>, 2024
            <br>
            <a href="https://ieeexplore.ieee.org/document/10375886">Paper</a> |
            <a href="https://github.com/HCPLab-SYSU/STKET">Code</a> |
            <a href="bib/TIP2024_STKET.bib" target="_blank">Cite</a>
            <p></p>
            <strong>TL;DR:</strong> A video scene graph generation approach that incorporates the prior spatial-temporal knowledge to learn better representations.
            <p></p>
          </td>
        </tr>
      -->

        <tr>  
          <td width="25%">
            <img src='media/TIP2024_STKET.png' width="160" height="124">
          </td>
          <td valign="top" width="75%">
                <papertitle>
                  Spatial–Temporal Knowledge-Embedded Transformer for Video Scene Graph Generation
                </papertitle>
              </a>
          <br>
              <strong>Tao Pu*</strong>, Tianshui Chen* &#9993;, Hefeng Wu, Yongyi Lu, Liang Lin
          <br>
              <em>IEEE Transactions on Image Processing (<strong>TIP</strong>)</em>, 2024
            <br>
            <a href="https://ieeexplore.ieee.org/document/10375886">Paper</a> |
            <a href="https://github.com/HCPLab-SYSU/STKET">Code</a> |
            <a href="bib/TIP2024_STKET.bib" target="_blank">Cite</a>
            <p></p>
            <strong>TL;DR:</strong> A video scene graph generation approach that incorporates the prior spatial-temporal knowledge to learn better representations.
            <p></p>
          </td>
        </tr>
    
        <tr>  
          <td width="25%">
            <img src='media/IJCV2024_HST.png' width="160" height="124">
          </td>
          <td valign="top" width="75%">
                <papertitle>
                  Heterogeneous Semantic Transfer for Multi-label Recognition with Partial Labels
                </papertitle>
              </a>
          <br>
              Tianshui Chen* &#9993;, <strong>Tao Pu*</strong>, Lingbo Liu, Yukai Shi, Zhijing Yang, Liang Lin
          <br>
              <em>International Journal of Computer Vision (<strong>IJCV</strong>)</em>, 2024
            <br>
            <a href="https://link.springer.com/article/10.1007/s11263-024-02127-2?utm_source=rct_congratemailt&utm_medium=email&utm_campaign=nonoa_20240715&utm_content=10.1007%2Fs11263-024-02127-2#citeas">Paper</a> |
            <a href="https://github.com/HCPLab-SYSU/HCP-MLR-PL">Code</a> |
            <a href="bib/IJCV2024_HST.bib" target="_blank">Cite</a> |
            <a href="https://zhuanlan.zhihu.com/p/684478294">知乎</a>
            <p></p>
            <strong>TL;DR:</strong> A weakly-supervised MLR approach that explores heterogeneous semantics inherently within multi-label images.
            <p></p>
          </td>
        </tr>

        <tr>  
          <td width="25%">
            <img src='media/TPAMI2022_CDFER.png' width="160" height="124">
          </td>
          <td valign="top" width="75%">
                <papertitle>
                  Cross-Domain Facial Expression Recognition: A Unified Evaluation Benchmark and Adversarial Graph Learning
                </papertitle>
              </a>
          <br>
              Tianshui Chen*, <strong>Tao Pu*</strong>, Hefeng Wu &#9993;, Yuan Xie, Lingbo Liu, Liang Lin
          <br>
              <em>IEEE Transactions on Pattern Analysis and Machine Intelligence (<strong>TPAMI</strong>)</em>, 2022
            <br>
            <a href="https://ieeexplore.ieee.org/document/9628054">Paper</a> |
            <a href="https://github.com/HCPLab-SYSU/CD-FER-Benchmark">Code</a> |
            <a href="bib/TPAMI2022_CDFER.bib" target="_blank">Cite</a> |
            <a href="https://zhuanlan.zhihu.com/p/353606619">知乎</a>
            <p></p>
            <strong>TL;DR:</strong> One of the largest CD-FER evaluation benchmarks that unifies the source/target datasets and feature extractors for existing algorithms.
            <p></p>
          </td>
        </tr>
  </table>
   
  <!-- Invited Talks --> 
  <table style="width:100%;border:0px;border-spacing:0px;border-collapse:collapse;margin-right:auto;margin-left:auto;">
    <heading>Invited Talks</heading>

    <br><br><br>

    <!--
    <tr onmouseout="hku_stop()" onmouseover="hku_start()">  
      <td width="25%">
        <div class="one">
        <div class="two" id = 'hku_shape'>
        <img src='media/talk_hku_teaser.jpg' width="180" height="100"></div>
        <img src='media/talk_hku_teaser.jpg' width="180" height="100"></div>
        </div>
        <script type="text/javascript">
        function hku_start() { 
        document.getElementById('hku_shape').style.opacity = "1";
        }
        function hku_stop() { 
        document.getElementById('hku_shape').style.opacity = "0"; 
        }
        hku_stop()
        </script>
      </td>
      <td valign="top" width="75%">
          <a href="files/talk_hku.pdf">
            <papertitle>2D Magic in a 3D World</papertitle></a>
      <br>
          <em><strong>Imperial College London</strong></em>, hosted by <a href="https://www.doc.ic.ac.uk/~ajd/">Andrew Davison</a>, 2024<br>
          <em><strong>Czech Technical University (CTU)</strong></em>, hosted by <a href="https://tsattler.github.io/">Torsten Sattler</a>, 2024<br>
          <em><strong>The University of Hong Kong (HKU)</strong></em>, hosted by <a href="https://www.kaihan.org/">Kai Han</a>, 2024
      <br>
        <a href="files/talk_hku.pdf">slides</a>
      </td>
    </tr>
    -->  
  </table>

 <!-- Honors & Awards -->
  <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
    <heading>Honors & Awards</heading>
      <tr>
        <td>
          <ul>
              <li> 中国科协青年人才托举工程博士生专项计划, 2024</li>
              <li> Graduate National Scholarship (博士研究生国家奖学金), 2024 </li>
              <li> Undergraduate National Scholarship (本科生国家奖学金), 2018 </li>
          </ul>
          <a href="javascript:toggleblock('honor&award')">---- show more ----</a>
              <div id="honor&award" style="display: none;">
                <ul>
                    <li> 广东省计算机学会优秀论文一等奖, 2024 </li>
                    <li> 粤港澳优秀研究生论文大赛三等奖, 2024 </li>
                </ul>
              </div>
        </td>
      </tr>
  </table>
      

  <!-- Academic Services -->
  <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
    <heading>Academic Services</heading>
      <tr>
        <td>
          <ul>
              <!-- <li> <strong>Publicity Chair</strong>: 3DV'25<br> -->
              <!-- <li> <strong>Area Chair</strong>: 3DV'24<br> -->
              <!-- <li> <strong>Workshop Organizer</strong>: <a href="https://opensun3d.github.io/">OpenSUN3D</a> at ICCV'23 (<a href="https://opensun3d.github.io/index_iccv23.html">1st</a>), CVPR'24 (<a href="https://opensun3d.github.io/index_cvpr2024.html">2nd</a>), and ECCV'24 (<a href="https://opensun3d.github.io/index.html">3rd</a>), <a href="https://focus-workshop.github.io/">FOCUS</a> at ECCV'24<br> -->
              <li> <strong>Conference Reviewer</strong>: CVPR, ICLR, NeurIPS, ICML, IJCAI, ACM MM, AAAI<br>
              <li> <strong>Journal Reviewer</strong>: TPAMI, TNNLS, TKDE, TOMM
          </ul>
        </td>
      </tr>
  </table>


    </td>
    </tr>
  </table>
  </body>
</html>
<!--  -->
